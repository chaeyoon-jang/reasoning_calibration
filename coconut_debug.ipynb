{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import re \n",
    "pattern = re.compile(r'.*?(?:\\. |\\n|$)', re.DOTALL)\n",
    "\n",
    "def extract_xml_cot(text: str) -> str:\n",
    "    think = text.rsplit(\"<think>\")[-1]\n",
    "    think = think.rsplit(\"<answer>\")[0]\n",
    "    return think \n",
    "\n",
    "#TODO: must be checked again...!!\n",
    "def make_latent_data(step, data):\n",
    "    \n",
    "    assert step > 0, \"step should be greater than 0\"\n",
    "    \n",
    "    temp_prompts, temp_labels, answer_labels = [], [], []\n",
    "    \n",
    "    for pred in data:\n",
    "        temp_prompts.append(pred.rsplit(\"<think>\", 1)[0] + \"<think>\")\n",
    "        ## ... <think><-| ... </think>\n",
    "        temp_labels.append(extract_xml_cot(pred))\n",
    "        ## <think> |-> ... </think> ... <-|<answer>\n",
    "        answer_labels.append(\"<answer>\" + pred.rsplit(\"<answer>\", 1)[-1])\n",
    "        ## |-><answer> ... </answer>|\n",
    "    \n",
    "    final_inputs, final_labels, num_steps = [], [], []\n",
    "    for t_p, t_l, o_l in zip(temp_prompts, temp_labels, answer_labels):\n",
    "        \n",
    "        temp_steps = [m.group() for m in pattern.finditer(t_l) if m.group()]\n",
    "        \n",
    "        ## Case 1) len(temp_steps) > step:\n",
    "        ## e.g., temp_steps = ['0xxxx. ', '1xxxx\\n', '2xxxx. ', '3xxxx\\n'] & step = 3\n",
    "        ## => <|start-latent|><|latent|><|latent|><|latent|><|end-latent|> + '3xxxx\\n' (temp_steps[step:])\n",
    "        ## final_inputs.append(t_p + \"<|start-latent|>\" + \"<|latent|>\" * step + \"<|end-latent|>\")\n",
    "        ## final_labels.append(temp_steps[step] + t_l.split(temp_steps[step:])[-1] + o_l)\n",
    "        \n",
    "        ## Case 2) len(temp_steps) < step:\n",
    "        ## e.g., temp_steps = ['0xxxx. ', '1xxxx\\n'] & step = 3\n",
    "        ## => <|start-latent|><|latent|><|latent|><|latent|> \n",
    "        ## step = len(temp_steps)\n",
    "        ## final_inputs.append(t_p + \"<|start-latent|>\" + \"<|latent|>\" * step + \"<|end-latent|>\")\n",
    "        ## final_labels.append(o_l)\n",
    "        \n",
    "        if step >= len(temp_steps):\n",
    "            final_inputs.append(t_p + \"<|start-latent|>\" + \"<|latent|>\" * len(temp_steps) + \"<|end-latent|>\")\n",
    "            final_labels.append(o_l)\n",
    "        \n",
    "        else:\n",
    "            final_inputs.append(t_p + \"<|start-latent|>\" + \"<|latent|>\" * step + \"<|end-latent|>\" )\n",
    "            final_labels.append(temp_steps[step] + t_l.split(temp_steps[step])[-1] + o_l)\n",
    "        \n",
    "        ## For checking,\n",
    "        num_steps.append(len(temp_steps))\n",
    "    \n",
    "    return final_inputs, final_labels, num_steps\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Pretraining Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "base_train = pd.read_csv(\"./data/processed/base/answer_kl/train.csv\")\n",
    "base_valid = pd.read_csv(\"./data/processed/base/answer_kl/valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>pred_answer</th>\n",
       "      <th>conf_labels</th>\n",
       "      <th>input_prompt</th>\n",
       "      <th>answer_label</th>\n",
       "      <th>confidence_input_prompt</th>\n",
       "      <th>confidence_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>For every 5 kangaroos, there is one koala, mea...</td>\n",
       "      <td>{question}\\n&lt;think&gt;First, we need to find out ...</td>\n",
       "      <td>60</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;answer&gt;There are 216 koalas and kangaroos alt...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;confidence&gt;60&lt;/confidence&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>Jennifer has 2*10 = &lt;&lt;2*10=20&gt;&gt;20 apples.\\nShe...</td>\n",
       "      <td>{question}\\n&lt;think&gt;\\nTo solve this problem, we...</td>\n",
       "      <td>90</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;answer&gt;\\nJennifer has 8 pears, 18 oranges, an...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;confidence&gt;90&lt;/confidence&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "\n",
       "                                         true_answer  \\\n",
       "0  For every 5 kangaroos, there is one koala, mea...   \n",
       "1  Jennifer has 2*10 = <<2*10=20>>20 apples.\\nShe...   \n",
       "\n",
       "                                         pred_answer  conf_labels  \\\n",
       "0  {question}\\n<think>First, we need to find out ...           60   \n",
       "1  {question}\\n<think>\\nTo solve this problem, we...           90   \n",
       "\n",
       "                                        input_prompt  \\\n",
       "0  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "\n",
       "                                        answer_label  \\\n",
       "0  <answer>There are 216 koalas and kangaroos alt...   \n",
       "1  <answer>\\nJennifer has 8 pears, 18 oranges, an...   \n",
       "\n",
       "                             confidence_input_prompt  \\\n",
       "0  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "\n",
       "              confidence_label  \n",
       "0  <confidence>60</confidence>  \n",
       "1  <confidence>90</confidence>  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_preds = list(base_train['input_prompt'])\n",
    "base_valid_preds = list(base_valid['input_prompt'])\n",
    "\n",
    "base_train_labels = list(base_train['answer_label'])\n",
    "base_valid_labels = list(base_valid['answer_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_START = \"<|start_latent|>\"\n",
    "LATENT_END   = \"<|end_latent|>\"\n",
    "LATENT_TOK   = \"<|latent|>\"\n",
    "\n",
    "ASSIST_TAG = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "def split_assistant(pred):\n",
    "    if ASSIST_TAG not in pred:\n",
    "        raise ValueError(\"assistant header not found\")\n",
    "    pre, post = pred.split(ASSIST_TAG, 1)\n",
    "    return pre + ASSIST_TAG, post.lstrip(\"\\n\")\n",
    "\n",
    "def mask_first_n_lines(text: str, n: int):\n",
    "    lines = text.splitlines(keepends=True)\n",
    "    n_lat = min(n, len(lines))\n",
    "    stub  = LATENT_START + LATENT_TOK * n_lat + LATENT_END\n",
    "    remain = \"\".join(lines[n_lat:])\n",
    "    return stub #+ remain\n",
    "\n",
    "def make_latent_data(step: int, samples):\n",
    "    inputs, labels, counts = [], [], []\n",
    "    for pred in samples:\n",
    "        prefix, assistant_body = split_assistant(pred)\n",
    "        counts.append(len(assistant_body.splitlines()))\n",
    "        \n",
    "        masked_assist = mask_first_n_lines(assistant_body, step)\n",
    "        \n",
    "        inputs.append(prefix + \"\\n\" + masked_assist)\n",
    "        \n",
    "        label_part = \"\\n\".join(assistant_body.splitlines()[step:])\n",
    "        labels.append(label_part)\n",
    "    return inputs, labels, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels, train_steps = make_latent_data(6, base_train_preds)\n",
    "valid_inputs, valid_labels, valid_steps = make_latent_data(6, base_valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1482"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "train_steps.index(int(np.max(train_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 17 Apr 2025\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This is a conversation between **User** and **Assistant**.\n",
      "The User asks a question, and the Assistant provides a solution.\n",
      "Before answering, the Assistant reasons through the problem step-by-step.\n",
      "The reasoning is enclosed within `<think> ... </think>`, and the final answer within `<answer> ... </answer>`.\n",
      "\n",
      "Example:\n",
      "{question}\n",
      "<think>{step-by-step reasoning}</think>\n",
      "<answer>{final answer}</answer>\n",
      "\n",
      "Now, respond to the following using the **exact same format**:\n",
      "Mark is running for an election and wins 70% of the votes in an area with 100,000 voters.  He got twice as many total votes in the remaining area.  How many votes did he get in total?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{question}\n",
      "<think>\n",
      "To solve this problem, we need to break it down into smaller steps. Let's start by identifying the information given:\n",
      "- Mark won 70% of the votes in one area with 100,000 voters.\n",
      "- He got twice as many total votes in the remaining area.\n",
      "- We need to find the total number of votes Mark received.\n",
      "\n",
      "First, let's calculate the number of votes Mark received in the first area. Since he won 70% of the votes, we can calculate this as 70% of 100,000.\n",
      "\n",
      "70% of 100,000 = 0.7 * 100,000 = 70,000 votes\n",
      "\n",
      "Now, we know that Mark got twice as many total votes in the remaining area. Let's call the number of votes in the remaining area 'x'. Since he got twice as many votes in this area, the total number of votes he received is the sum of the votes in both areas.\n",
      "\n",
      "Total votes = votes in the first area + votes in the remaining area\n",
      "Total votes = 70,000 + x\n",
      "\n",
      "We also know that the total number of votes is equal to 100,000 (the total number of voters in the area where he didn't win). So, we can set up the equation:\n",
      "\n",
      "70,000 + x = 100,000\n",
      "\n",
      "To solve for x, we subtract 70,000 from both sides of the equation:\n",
      "\n",
      "x = 100,000 - 70,000\n",
      "x = 30,000\n",
      "\n",
      "Now that we know the number of votes in the remaining area, we can calculate the total number of votes Mark received:\n",
      "\n",
      "Total votes = 70,000 + 30,000\n",
      "Total votes = 100,000\n",
      "\n",
      "However, this is not possible since Mark got 70% of the votes in one area and twice as many in the remaining area. This means the total number of votes in the remaining area must be more than 30,000. Let's re-evaluate the problem.\n",
      "\n",
      "Let's say the number of votes in the remaining area is 'x'. Since Mark got twice as many votes in this area, the total number of votes he received is the sum of the votes in both areas:\n",
      "\n",
      "Total votes = 70,000 + 2x\n",
      "\n",
      "We also know that the total number of votes is equal to 100,000. So, we can set up the equation:\n",
      "\n",
      "70,000 + 2x = 100,000\n",
      "\n",
      "To solve for x, we subtract 70,000 from both sides of the equation:\n",
      "\n",
      "2x = 30,000\n",
      "\n",
      "Divide both sides by 2:\n",
      "\n",
      "x = 15,000\n",
      "\n",
      "Now that we know the number of votes in the remaining area, we can calculate the total number of votes Mark received:\n",
      "\n",
      "Total votes = 70,000 + 2x\n",
      "Total votes = 70,000 + 2(15,000)\n",
      "Total votes = 70,000 + 30,000\n",
      "Total votes = 100,000\n",
      "\n",
      "However, this is still not possible since Mark got 70% of the votes in one area and twice as many in the remaining area. Let's re-evaluate the problem again.\n",
      "\n",
      "Let's say the number of votes in the remaining area is 'x'. Since Mark got twice as many votes in this area, the total number of votes he received is the sum of the votes in both areas:\n",
      "\n",
      "Total votes = 70,000 + 2x\n",
      "\n",
      "We also know that the total number of votes is equal to 100,000. So, we can set up the equation:\n",
      "\n",
      "70,000 + 2x = 100,000\n",
      "\n",
      "To solve for x, we subtract 70,000 from both sides of the equation:\n",
      "\n",
      "2x = 30,000\n",
      "\n",
      "Divide both sides by 2:\n",
      "\n",
      "x = 15,000\n",
      "\n",
      "Now that we know the number of votes in the remaining area, we can calculate the total number of votes Mark received:\n",
      "\n",
      "Total votes = 70,000 + 2x\n",
      "Total votes = 70,000 + 2(15,000)\n",
      "Total votes = 70,000 + 30,000\n",
      "Total votes = 100,000\n",
      "\n",
      "However, this is still not possible since Mark got 70% of the votes in one area and twice as many in the remaining area. Let's re-evaluate the problem again.\n",
      "\n",
      "Let's say the number of votes in the remaining area is 'x'. Since Mark got twice as many votes in this area, the total number of votes he received is the sum of the votes in both areas:\n",
      "\n",
      "Total votes = 70,000 + 2x\n",
      "\n",
      "We also know that the total number of votes is equal to 100,000. So, we can set up the equation:\n",
      "\n",
      "70,000 + 2x = 100,000\n",
      "\n",
      "To solve for x, we subtract 70,000 from both sides of the equation:\n",
      "\n",
      "2x = 30,000\n",
      "\n",
      "Divide both sides by\n"
     ]
    }
   ],
   "source": [
    "print(base_train['input_prompt'][1482])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 8.893547427891763\n",
      "Median: 8.0\n",
      "25% Quantile: 5.0\n",
      "75% Quantile: 11.0\n",
      "MAX: 95\n",
      "==============================\n",
      "Mean: 8.682730923694779\n",
      "Median: 8.0\n",
      "25% Quantile: 5.0\n",
      "75% Quantile: 11.0\n",
      "MAX: 48\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean = np.mean(train_steps)\n",
    "median = np.median(train_steps)\n",
    "std = np.std(train_steps)\n",
    "q25 = np.quantile(train_steps, 0.25)\n",
    "q75 = np.quantile(train_steps, 0.75)\n",
    "max = np.max(train_steps)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Median: {median}\")\n",
    "#print(f\"Std: {std}\")\n",
    "print(f\"25% Quantile: {q25}\")\n",
    "print(f\"75% Quantile: {q75}\")\n",
    "print(f\"MAX: {max}\")\n",
    "\n",
    "print(\"=\"*30)\n",
    "\n",
    "mean = np.mean(valid_steps)\n",
    "median = np.median(valid_steps)\n",
    "std = np.std(valid_steps)\n",
    "q25 = np.quantile(valid_steps, 0.25)\n",
    "q75 = np.quantile(valid_steps, 0.75)\n",
    "max = np.max(valid_steps)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Median: {median}\")\n",
    "#print(f\"Std: {std}\")\n",
    "print(f\"25% Quantile: {q25}\")\n",
    "print(f\"75% Quantile: {q75}\")\n",
    "print(f\"MAX: {max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0 -> 1\n",
    "## 1 -> 2\n",
    "## 2 -> 3\n",
    "## 4,5,6 -> 4\n",
    "## 7,8,9 -> 5\n",
    "## 10,11,12 -> 6\n",
    "## 13,14,15 -> 7\n",
    "## 16,17,18 -> 8\n",
    "## 19,20,21 -> 9\n",
    "## 22,23,24 -> 111\n",
    "\n",
    "stages = [i+1 for i in range(9)] + [95]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in [i+1 for i in range(9)] + [95]:\n",
    "    \n",
    "    train_inputs, train_labels, train_steps = make_latent_data(step, base_train_preds)\n",
    "    valid_inputs, valid_labels, valid_steps = make_latent_data(step, base_valid_preds)\n",
    "    \n",
    "    train_df = pd.DataFrame({\n",
    "        'input_prompt': train_inputs, \n",
    "        'answer_label': [l1 + l2 for l1, l2 in zip(train_labels, base_train_labels)], \n",
    "    })\n",
    "    \n",
    "    valid_df = pd.DataFrame({\n",
    "        'input_prompt': valid_inputs, \n",
    "        'answer_label': [l1 + l2 for l1, l2 in zip(valid_labels, base_valid_labels)],\n",
    "    })\n",
    "    \n",
    "    train_df.to_csv(f\"./data/prev/prev/coconut/stage_{step}_train.csv\", index=False)\n",
    "    valid_df.to_csv(f\"./data/prev/prev/coconut/stage_{step}_valid.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_prompt</th>\n",
       "      <th>answer_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;answer&gt;There are 216 koalas and kangaroos alt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;answer&gt;\\nJennifer has 8 pears, 18 oranges, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        input_prompt  \\\n",
       "0  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "\n",
       "                                        answer_label  \n",
       "0  <answer>There are 216 koalas and kangaroos alt...  \n",
       "1  <answer>\\nJennifer has 8 pears, 18 oranges, an...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 17 Apr 2025\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This is a conversation between **User** and **Assistant**.\n",
      "The User asks a question, and the Assistant provides a solution.\n",
      "Before answering, the Assistant reasons through the problem step-by-step.\n",
      "The reasoning is enclosed within `<think> ... </think>`, and the final answer within `<answer> ... </answer>`.\n",
      "\n",
      "Example:\n",
      "{question}\n",
      "<think>{step-by-step reasoning}</think>\n",
      "<answer>{final answer}</answer>\n",
      "\n",
      "Now, respond to the following using the **exact same format**:\n",
      "A busy restaurant is counting how many customers they had during that Friday to try to predict how many they might get on Saturday. During breakfast, they had 73 customers. During lunch, they had 127 customers. During dinner, they had 87 customers. If they predict they'll get twice the amount of customers on Saturday as they had on Friday, how many customers do they predict they will get?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|end_latent|>\n"
     ]
    }
   ],
   "source": [
    "print(train_df['input_prompt'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<answer>\n",
      "64\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "print(train_df['answer_label'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "valid_data = pd.read_csv(\"./data/prev/prev/coconut/stage_8_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/chaeyun-jang/.conda/envs/llm_ft/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.68s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "from src.llm_model_utils import (\n",
    "    create_model,\n",
    "    create_tokenizer\n",
    "    )   \n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = create_tokenizer(model_name)\n",
    "model = create_model(\n",
    "    model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    use_int8=False,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_tokens(\"<|start-latent|>\")\n",
    "tokenizer.add_tokens(\"<|end-latent|>\")\n",
    "tokenizer.add_tokens(\"<|latent|>\")\n",
    "\n",
    "latent_id = tokenizer.convert_tokens_to_ids(\"<|latent|>\")\n",
    "start_id = tokenizer.convert_tokens_to_ids(\"<|start-latent|>\")\n",
    "end_id = tokenizer.convert_tokens_to_ids(\"<|end-latent|>\")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "embeddings = model.get_input_embeddings()\n",
    "target_id = tokenizer.convert_tokens_to_ids(\"<<\")\n",
    "\n",
    "for token_id in [latent_id, start_id, end_id]:\n",
    "    target_embedding = embeddings.weight.data[target_id] \n",
    "    embeddings.weight.data[token_id] = target_embedding\n",
    "    lm_head = model.lm_head\n",
    "    lm_head.weight.data[token_id] = lm_head.weight.data[target_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/chaeyun-jang/.conda/envs/llm_ft/lib/python3.12/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.query.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.query.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.query.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.query.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.query.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "from src.peft_utils import get_lora_model\n",
    "\n",
    "query_peft_dir = \"/mnt/home/chaeyun-jang/reasoning_calibration/logs/2025-05-10T18-27-05\"\n",
    "\n",
    "model = get_lora_model(\n",
    "    model,\n",
    "    peft_id_or_dir=query_peft_dir,\n",
    "    is_trainable=False,\n",
    "    adapter_name=\"query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.coconut import Coconut\n",
    "\n",
    "coconut_model = Coconut(model, latent_id, start_id, end_id, tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(valid_data['input_prompt'][1], return_tensors=\"pt\").to('cuda')\n",
    "input_ids['position_ids'] = torch.tensor(list(range(len(input_ids['input_ids'][0])))).unsqueeze(0).to('cuda')\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "output = coconut_model.generate(**input_ids, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 17 Apr 2025\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This is a conversation between **User** and **Assistant**.\n",
      "The User asks a question, and the Assistant provides a solution.\n",
      "Before answering, the Assistant reasons through the problem step-by-step.\n",
      "The reasoning is enclosed within `<think> ... </think>`, and the final answer within `<answer> ... </answer>`.\n",
      "\n",
      "Example:\n",
      "{question}\n",
      "<think>{step-by-step reasoning}</think>\n",
      "<answer>{final answer}</answer>\n",
      "\n",
      "Now, respond to the following using the **exact same format**:\n",
      "James creates a media empire.  He creates a movie for $2000.  Each DVD cost $6 to make.  He sells it for 2.5 times that much.  He sells 500 movies a day for 5 days a week.  How much profit does he make in 20 weeks?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|end_latent|>\n"
     ]
    }
   ],
   "source": [
    "print(valid_data['input_prompt'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selling price per DVD = 2.5 * $6 = $15\n",
      "Next, we need to find the total revenue per day. James sells 500 movies per day, so we multiply the selling price by the number of movies sold.\n",
      "Total revenue per day = 500 * $15 = $7500\n",
      "Since James sells 500 movies a day for 5 days a week, we multiply the total revenue per day by 5.\n",
      "Total revenue per week = $7500 * 5 = $37,500\n",
      "Now, we need to find the total revenue for 20 weeks. We multiply the total revenue per week by 20.\n",
      "Total revenue for 20 weeks = $37,500 * 20 = $750,000\n",
      "Next, we need to find the total cost for 20 weeks. James spends $6 per DVD, and he sells 500 DVDs per day for 5 days a week. So, the total cost per week is:\n",
      "Total cost per week = 500 * $6 = $3000\n",
      "Total cost for 20 weeks = $3000 * 20 = $60,000\n",
      "Finally, we can find the profit by subtracting the total cost from the total revenue.\n",
      "Profit = Total revenue - Total cost\n",
      "Profit = $750,000 - $60,000\n",
      "Profit = $690,000\n",
      "</think><answer>\n",
      "$690,000\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "print(valid_data['answer_label'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 17 Apr 2025\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This is a conversation between **User** and **Assistant**.\n",
      "The User asks a question, and the Assistant provides a solution.\n",
      "Before answering, the Assistant reasons through the problem step-by-step.\n",
      "The reasoning is enclosed within `<think>... </think>`, and the final answer within `<answer>... </answer>`.\n",
      "\n",
      "Example:\n",
      "{question}\n",
      "<think>{step-by-step reasoning}</think>\n",
      "<answer>{final answer}</answer>\n",
      "\n",
      "Now, respond to the following using the **exact same format**:\n",
      "James creates a media empire.  He creates a movie for $2000.  Each DVD cost $6 to make.  He sells it for 2.5 times that much.  He sells 500 movies a day for 5 days a week.  How much profit does he make in 20 weeks?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|latent|><|end_latent|> \n",
      "<think>\n",
      "To find the profit James makes, we need to calculate the total revenue and subtract the total cost.\n",
      "First, we need to find the cost of producing 500 movies in a day. \n",
      "The cost of producing one DVD is $6, so the cost of producing 500 DVDs is 500 x $6 = $3000.\n",
      "Next, we need to find the selling price of one DVD. \n",
      "The selling price is 2.5 times the cost, so the selling\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
